\chapter{Data Collection}

\section{The Electome}

The Electome is a large, collaborative, and ongoing effort in the Laboratory for Social Machines that seeks to analyze the ``competition of ideas'' in the upcoming 2016 elections. It does so by using techniques in natural language processing, machine learning, and network analysis to make sense of ``big data'' collected from two main sources: traditional media (online versions of news publications) and social media (Twitter) \cite{vvr_electome2016}. 

The foundations of this thesis, which emerged from the Electome, are grounded in the former dataset, although only a portion of the data collected is analyzed in this study.

\section{Story Collection} 

News articles from 14 different news publications were systematically collected every hour from RSS feeds beginning from January 2015. The outlets tracked are:
 
\begin{itemize}
  \item CNN
  \item Fox News
  \item The Wall Street Journal
  \item ProPublica
  \item Politico
  \item McClatchy
  \item The Washington Post
  \item Buzzfeed (News only)
  \item NPR
  \item The Huffington Post
  \item The Associated Press
  \item Reuters
  \item The New York Times
  \item The Los Angeles Times
\end{itemize}

The above outlets were chosen to form a diverse subset of the current U.S. news ecosytem, including a combination of private and public, liberal and conservative, legacy and new media publications. Also included are wire services and a mix of media delivery formats for which the outlet is known (radio, television, print, or web).

Steps to collect the news stories were as follows:

 \begin{enumerate}
   \item For each news publication:
   \begin {enumerate}
        \item Use regular expressions to extract all RSS feed urls for a news site.
        \item For each RSS feed:
        \begin {enumerate}
            \item Parse feed using open source xml reader library, Feedparser.
            \item For each link to a story in the feed:             
            \begin {enumerate}
                \item Parse html using Beautiful Soup 3 (an open source python library)
                \item Insert headline, authors, story text, publication date and retrieval date into an SQL database.

            \end {enumerate}
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

Data depulication (by story url and headline) is then performed to ensure only one copy of each article is in the database. This step is necessary as articles from wire services often appear across many outlets and effect aggregate text analsyis.

On average, 2,000 stories are collected per day across all outlets. However, volume follows a consistent pattern of fluctuation depending on weekday, ranging from approximately 1,000 to 3,000 stories.

[INSERT HERE GRAPH OF NEWS STORIES VOLUME BY WEEKDAY]

As of March 1st, 2016, there were 855,000 stories collected in the database and 43,000 journalists.


For the purposes of this study, stories were examined from five outlets: 

\begin{itemize}
  \item CNN
  \item Fox News  
  \item The New York Times
  \item The Wall Street Journal
  \item The Associated Press 
\end{itemize}

because:   
and cite Pew

\section{Election Classification}

Acknowledge Prashanth

\section{Article Topic Classification}
(This part is prashanths-- so cite)
\begin{itemize}
  \item Income Inequality
  \item Environment/Energy
  \item Jobs/Employment
  \item Guns
  \item Racial Issues
  \item Foreign Policy/National Security
  \item LGBT Issues
  \item Ethics
  \item Education
  \item Financial Regulation
  \item Budget/Taxation
  \item Veterans
  \item Campaign Finance
  \item Surveillance/Privacy
  \item Drugs
  \item Justice
  \item Abortion
  \item Immigration
  \item Trade
  \item Health Care
  \item Economy
  \item Other 
\end{itemize}
 


\section{Flesch-Kincaid Readability Tests} 

















