\chapter{Data Collection}

\section{The Electome}

The Electome is a large, collaborative, and ongoing effort in the Laboratory for Social Machines that seeks to analyze the ``competition of ideas'' in the upcoming 2016 elections. It does so by using techniques in natural language processing, machine learning, and network analysis to make sense of ``big data'' collected from two main sources: traditional media (online versions of news publications) and social media (Twitter) \cite{vvr_electome2016}. 

The foundations of this thesis, which emerged from the Electome, are grounded in the former dataset, although only a portion of the data collected is analyzed in this study.

\section{Story Collection} 

News articles from 14 different news publications were systematically collected every hour from RSS feeds beginning from January 2015. The outlets tracked are:
 

\begin{multicols}{2}
    \begin{itemize}
    \itemsep-1em 
      \item CNN
      \item Fox News
      \item The Wall Street Journal
      \item ProPublica
      \item Politico
      \item McClatchy
      \item The Washington Post
      \item Buzzfeed (News only)
      \item National Public Radio (NPR)
      \item The Huffington Post
      \item The Associated Press
      \item Reuters
      \item The New York Times
      \item The Los Angeles Times
    \end{itemize}
\end{multicols}

The above outlets were chosen to form a diverse subset of the current U.S. news ecosytem, including a combination of private and public, liberal and conservative, legacy and new media publications. Also included are wire services and a mix of media delivery formats for which the outlet is known (radio, television, print, or web).

Steps to collect the news stories were as follows:

 \begin{enumerate}
   \item For each news publication:
   \begin {enumerate}
        \item Use regular expressions to extract all RSS feed urls for a news site.
        \item For each RSS feed:
        \begin {enumerate}
            \item Parse feed using open source xml reader library, Feedparser.
            \item For each link to a story in the feed:             
            \begin {enumerate}
                \item Parse html using Beautiful Soup 3 (an open source python library)
                \item Insert headline, authors, story text, publication date and retrieval date into an SQL database.

            \end {enumerate}
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

Data depulication (by story url and headline) is then performed to ensure only one copy of each article is in the database. This step is necessary as articles from wire services often appear across many outlets and effect aggregate text analsyis.

On average, 2,000 stories are collected per day across all outlets. However, volume follows a consistent pattern of fluctuation depending on weekday, ranging from approximately 1,000 to 3,000 stories.

[INSERT HERE GRAPH OF NEWS STORIES VOLUME BY WEEKDAY]

As of March 1st, 2016, there were 855,000 stories collected in the database and 43,000 journalists.


For the purposes of this study, stories were examined from five outlets: 

\begin{itemize}
\itemsep-1em 
  \item CNN
  \item Fox News  
  \item The New York Times
  \item The Wall Street Journal
  \item The Associated Press 
\end{itemize}

because:   
and cite Pew

\section{Election Classification}
[How do I cite Prashanth's unpublished work?]

\section{Article Topic Classification}
[How do I cite Prashanth's unpublished work?]
 

Election Classifier
The election classifier is a binary classifier which takes a news article as input and determines whether it is about the 2016 US election or not. Since news articles usually con- tain clean and structured language, they can easily be classi- fied as election-related using Bag-of-Word (BoWs) features. We used the chi-square test for feature selection. Chi-square measures the lack of independence between a term in an ar- ticle and a class (in this case the election). High scores on chi-square indicate that the null hypothesis of independence should be rejected and thus that the occurrence of the term and class are dependent. The features are ranked based on their scores and the top 20,000 features form the vocabulary for the binary classifier. Next, using scikit-learn (Pedregosa et al. 2011)–a Python machine learning library– a binary Maximum Entropy (MaxEnt) text classifier (Nigam, Laf- ferty, and McCallum 1999) is trained on a balanced dataset of 1,000 manually labelled news articles. The classifier was evaluated on a separate balanced test set of 300 articles, with



\begin{multicols}{2}
    \begin{itemize}
    \itemsep-1em 
      \item Income Inequality
      \item Environment/Energy
      \item Jobs/Employment
      \item Guns
      \item Racial Issues
      \item Foreign Policy/National Security
      \item LGBT Issues
      \item Ethics
      \item Education
      \item Financial Regulation
      \item Budget/Taxation
      \item Veterans
      \item Campaign Finance
      \item Surveillance/Privacy
      \item Drugs
      \item Justice
      \item Abortion
      \item Immigration
      \item Trade
      \item Health Care
      \item Economy
      \item Other 
    \end{itemize}
\end{multicols}
 


\section{Flesch-Kincaid Readability Tests} 
In this study, we focus primarily on the Flesch-Kincaid (F-K) tests for estimating text readability. Originally developed for the U.S. Navy in 1975 for assessing the difficulty of technical manuals, the F-K reading level corresponds roughly to U.S. grade level and the reading ease score is inversely proportional to the grade level on a scale from 0 to approximately 120 \cite{kincaid1975derivation}.

We chose the F-K tests over other comparable ones due to its popularity in educational assessment and other applications, including in legislation. For example, it is required by law in Florida that life insurance policies have a Flesch reading ease of 45 or greater (less than 12th grade in reading level) \cite{Statu37online}. The F-K tests are also bundled in many common word processing services, including Microsoft Office Word. As a comparison, basic article analysis is also computed using the Gunning fog index (see Section 5.2.1).

The formula for Flesch reading ease is as follows:

$$206.835 - 1.015 \left( \frac{\mbox{total words}}{\mbox{total sentences}} \right) - 84.6 \left( \frac{\mbox{total syllables}}{\mbox{total words}} \right)$$

And for reading grade level:

$$0.39 \left ( \frac{\mbox{total words}}{\mbox{total sentences}} \right ) + 11.8 \left ( \frac{\mbox{total syllables}}{\mbox{total words}} \right ) - 15.59$$
 
The two formulas are not directly comparable due to the difference in weighting factors. For ease of metaphor, we use the grade level tests in our analysis. Syllable length is highly weighted in this formula, so it is possible to generate a story of very high reading level that consists of a single word in a single sentence (the longest English word, \emph{pneumonoultramicroscopicsilicovolcanoconiosi}, a type of lung disease, has a reading grade level of 197.2), which is a limitation of the method, since texts with polysyllabic words are not always necessarily more difficult to read.
  