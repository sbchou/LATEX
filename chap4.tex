\chapter{Study}

\section{Hypotheses}
 
This study sets out to tackle the question of reading level's effect in perceptions of news bias. In particular, how does it compare to factors associated with latent biases of the reader, such as media brand?

Although the body of literature in Chapter 2 examines the theories behind partisanship and media branding, little work has been done to compare those contextual effects with the effects of \emph{content} within a story, such as language use.

We ask the following research question concerning story \emph{content}:

\begin{itemize}
\item \textbf{Q1:} Does reading level have an effect on a reader's reported level of trust in an article?
\end{itemize}

And make the following hypothesis concerning story \emph{context}:

\begin{itemize}
\item \textbf{H1:} Media brand effects are more important than actual source and contents of an article. 
\end{itemize}

To examine \textbf{H1}, we expect to see significant brand effects of publications \emph{even when} stories are in fact identical. This brand manipulation builds off prior work by Baum in the study of television reporting \cite{baum2008eye}.

Finally, we expect to observe hostile media effects \cite{vallone1985hostile}. In other words:

\begin{itemize}
\item \textbf{H2:} We will observe interactions between the party of the reader and the perceived source of the story.
\end{itemize}




% This thesis tests five hypothesis.

% We explore two novel hypotheses testing reading level effects:

% \begin{itemize}
% \item \textbf{H1}: High reading level stories increase trust in the story.
% \item \textbf{H2}: However, they decrease perceptions of fairness. 
% \end{itemize}

% As well as two comparing the role of \emph{content} versus \emph{context}:

% \begin{itemize}
% \item \textbf{H3}: Media brand has a stronger role in determining story trust than reading level. 
% \item \textbf{H4}: Media brand has a stronger role in determining story fairness than reading level.
% \end{itemize}

% And verifying former theories, we expect that:
% \begin{itemize}
% \item \textbf{H5}: Stories shown to be from outlets of aligned political party score significantly higher on both trust and fairness than those of the opposite.
% %\item \textbf{H6}: Stories about candidates opposite to the readers preferred candidate score significantly lower in fairness, regardless of outlet.
% \end{itemize}

% We hypothesis \textbf{H1}: that high reading level of stories increase trust based off the work from Weisberg et. al showing that neuroscience explanations sway believability of scientific explanations, due to the field-specific nature of political reporting \cite{weisberg2008seductive}.

% Conversely, we predict \textbf{H2} that it creates a decrease in the perception of fairness in the story, due to the polarizing nature of political news and the fact that more complex stories could cause a partisan individual to more quickly reject what appears as an onslaught of conflicting information \cite{cacioppo1979effects}.
 
%  We hypothesize \textbf{H3} and \textbf{H4} that media brand effects outweigh content in determining both trust and fairness.

%  Finally, we expect to see hostile media effects (\textbf{H5}) to emerge.

\section{Experimental Design}
%For the second study, our experiment was revised to have a 4 x 2 mixed-factorial design.
Our experiment has a 4 x 2 mixed-factorial design.
 
\begin{center}
\begin{table}[!htbp]
\begin{tabular}{ | m{5em} | m{7em}| m{7em} | m{7em} | m{7em} | } 
 \hline
  & \textbf{Source: None} & \textbf{Source: AP} & \textbf{Source: Fox} & \textbf{Source: CNN} \\
 \hline
 \textbf{High Reading Level} & Clinton, Cruz, Sanders, Trump & Clinton, Cruz, Sanders, Trump & Clinton, Cruz, Sanders, Trump & Clinton, Cruz, Sanders, Trump  \\ 
 \textbf{Low Reading Level} & Clinton, Cruz, Sanders, Trump & Clinton, Cruz, Sanders, Trump & Clinton, Cruz, Sanders, Trump & Clinton, Cruz, Sanders, Trump \\ 
 \hline
\end{tabular}
\caption{Main Study Design}
\label{study2}
\end{table}
\end{center}
\newpage

% \begin{itemize}
%   \item Source (4 levels: None, AP, CNN, Fox)
%   \item Candidate (4 levels: Clinton, Cruz, Sanders, Trump)
%   \item Reading Level (2 levels: High, Low)
% \end{itemize}

In this study, reading level of articles and candidates featured in the articles were treated as within-subject variables, and the source of the story between-subjects.

% This time, we reduced the number of stories to N=8, and also changed reading level from a 3-level to 2-level variable (low, high) for clarity.

% Most significantly, since we observed some significant effect from disclosing source to the reader in Study 1, we added a manipulation in this experiment to further study the effect of revealing the source:

% Following Baum's research in showing the effects of media brands and reader bias by manipulating reported brands, all eight stories in Study 2 were in fact written by the Associated Press, however, we manipulated the source shown to the reader \cite{baum2008eye}. In group A, readers were shown the headline and text of the story with no other context. In group B, readers were additionally shown that the story was from the Associated Press (true label). In groups C and D, readers were shown that the story was from CNN and Fox News, respectively.

Each participant reads eight stories, two each of high and low reading level per candidate. However, to examine effects of media brands and reader bias, we manipulate the source attributed to the story, building off Baum's research in media brands and television reporting \cite{baum2008eye}.

All eight stories in this study were in fact written by the Associated Press, however, readers are divided into four groups receiving different labels. In group A, readers were shown the headline and text of the story with no other context. In group B, readers were additionally shown that the story was from the Associated Press (true label). In groups C and D, readers were shown that the story was from CNN and Fox News, respectively.

This setup was created to eliminate some of the confounding effects from using stories from different sources (writing style, focus of content, slant, etc.), while directly observing the effect of revealing a specific source to the reader. The Associated Press was chosen as the source of the stories as it is the highest circulation newswire service in the United States, and has 14,000 members that use its content \cite{apFAQ}. Notably, both CNN and Fox News publish content in full or part from the Associated Press, although the specific stories chosen had not been published in full by either to avoid bias.

After each article, we ask the reader to rank the trustworthiness of the story on a 5-point Likert scale.

\subsection{Dataset} 

Eight stories were chosen for this study: two (high and low reading level) for four presidential candidates. All eight stories were written by reporters from the Associated Press (although they may have been republished elsewhere).

To form the initial pool of articles, we programatically collected stories from the RSS feeds of the Associated Press online every hour over a 3-month period: from January 1, 2016 of the election year to March 1, 2016 (Super Tuesday). 
Based on the results of Super Tuesday, we then selected four candidates for this study by delegate count: Hillary Clinton (1,279), Bernie Sanders (1,027), Donald Trump (743), and Ted Cruz (517) \cite{March45online}.

News articles were then separated into single-candidate stories (i.e. articles featuring primarily one candidate in the headline) to be able to measure more clearly the perceived bias per candidate. This was done programatically using regular expressions to determine if a headline contained one candidate and one candidate only. A dictionary of related names was created to make sure that stories were correctly categorized (i.e. ``Hillary'', ``Clinton'', and ``Hillary Clinton'' were to be categorized as pertaining to ``Hillary Clinton'' but not if preceded by ``Bill'').

Reading level cutoffs were then made by taking the bottom and top 25\% percentile of Flesch-Kincaid scores for each candidate. From the set of stories that made the cutoff, we formed pairs of high and low reading level stories from each topic. The topic with the highest distance between reading level in the pair was chosen for each candidate.

% put histograms of reading level with cutoff lines
 
\subsection{Survey Design}
We designed four surveys (1 per group) on the platform CrowdFlower. Each participant was randomnly assigned to a group and could not take the survey more than once. The eight stories were shown (in a randomized order), and each story was followed up by a question of trustworthiness.

\begin{figure}[H] 
\centering
  \frame{\includegraphics[width=0.45\textwidth]{study2_qs}}
  \caption{Scoring Questions for Survey}
\end{figure}

The survey concluded with an abbreviated standard demographic survey as well as a political affiliation survey adapted from Pew's standard polling survey \cite{Pew-demographics}. These more personal questions were placed at the end to prevent priming readers beforehand.


\begin{figure}[h!] 
\centering
  \frame{\includegraphics[width=0.45\textwidth]{demographic_qs}}
  \caption{Demographic Questions for Survey}
\end{figure}

% Finally, we asked readers to report whether or not they might have read the stories before.

\begin{figure}[H] 
\centering
  \frame{\includegraphics[width=0.45\textwidth]{survey_comments}}
  \caption{Comments for Survey}
\end{figure}

We ran the survey of a duration of  hours and had 40 participants sign up per group, for a total of 160 participants.

\subsection{Quality Control}

CrowdFlower has a built-in ``Test Question'' feature that allows for the rejection of a annotator whose answers to specific questions do not lie within a threshold (default 70\%) of the ``correct'' answer or whose answers lay outside the standard variation compared to others.

However, since the questions we asked were by nature subjective and therefore outliers and disagreements in answers could imply signal rather than noise, we chose to monitor for quality using other metrics instead. CrowdFlower was not designed explicitly for survey-like tasks, and therefore there were no options for different screening methods or questions. Gold Questions on the platform are selected by the creator within the set of all questions being recorded.

Because of this, we monitored quality of results in two ways:

First, by setting a minimum of time of 360 seconds to complete the task of reading 5 stories for a task to be accepted.

Second, by selecting only Level 3 contributors on CrowdFlower as suggested on their website for handling survey-like tasks \cite{CrowdFlower-guide}.

Level 3 contributors are described as those who ``have completed over a hundred Test Questions across hundreds of different Job types, and have a near perfect overall Accuracy'' \cite{CrowdFlower-levels}. This is the highest category of contributor.
 
Users were also only allowed to answer the set of questions once. 

\$0.80 was given per survey, as suggested by MIT Committee on the Use of Humans as Experimental Subjects. The average response time per survey was 09:20 min.


\section{Survey Results}
In this section, we report descriptive analyses on the nature of the data collected.

\subsection{Demographics}
\begin{figure}[H] 
\centering 
  \includegraphics[width=0.8\textwidth]{gender_study2} 
  \includegraphics[width=0.8\textwidth]{age_study2} 
  %\includegraphics[width=0.32\textwidth]{location_study2} 
  \caption{Demographics of Participants
    \label{fig:demographics2}}
\end{figure}

Gender of participants were majority male. We had 758 male participants, 496 female participants and 24 signed up as ``other'' (it is possible that those who did not wish to identify chose the ``other'' category). 

Majority of participants were also in age group 18-29.
  

\subsection{Location}

Our participants represented a wide variety of geographic locations in the US.

\begin{figure}[H]  
\centering  
  \includegraphics[width=0.9\textwidth]{location_study2} 
  \caption{Locations of Participants
    \label{fig:locations2}}
\end{figure}

\subsection{Political Affiliation}

More than twice as many Democrats than Republicans particpated in our study, which also was reflected in candidate preference disparaties. 

\begin{figure}[H]  
\centering 
  \includegraphics[width=.8\textwidth]{party_study2} 
  \includegraphics[width=.8\textwidth]{voting_for_study2}  
  \caption{Political Affiliations of Participants
    \label{fig:political2}}
\end{figure}

 
  
% See figure~\ref{fig:demographics2} on page \pageref{fig:demographics2}

% See figure~\ref{fig:locations2}
% See figure~\ref{fig:party2}

\newpage
\subsection{Trust}
From a scale of -2 (Strongly Disagree) to 2 (Strongly Agree), on average, most stories were deemed trustworthy for all groups with a mean score of 0.55 (between Neutral and Agree). 

\begin{figure}[H]  
\centering 
  \includegraphics[width=.8\textwidth]{trust}  
  \caption{Overall Trust Ratings
    \label{fig:trust}}
\end{figure}

However, the distrubtions for candidates varied. Stories about Sanders were seen as most trustworthy across all participants with a mean score of 0.66, followed by Cruz (0.63), Clinton (0.53), and Trump (0.40).
 

On average, men were slightly less likely to trust stories (average of 0.54) than women (0.60) and others. 

Those in age group 30-49 were most likely to find stories trustworthy (average 0.675), followed by those in age group 50-64 (average 0.083), then those 18-29 (0.49), then those 64+ (0.083).

For distributions of trust by age group, see figure~\ref{fig:trust-by-age} on page \pageref{fig:trust-by-age}.

On average, stories were more trusted by Democrats (0.69 mean).
 
% \subsection{Fairness}
% From a scale of -2 (Strongly Disagree) to 2 (Strongly Agree), on average, most stories were deemed fair for all groups with a mean score of 0.57 (between Neutral and Agree). 

% \begin{figure}[H]  
% \centering 
%   \includegraphics[width=.8\textwidth]{fair}  
%   \caption{Overall Fairness Ratings 
%   \label{fig:fairness}}
% \end{figure}

% For distribution of all scores, see figure~\ref{fig:fair} on page \pageref{fig:fair}.

% Perceptions of fair treatment of candidates in stories diverged from the trust ratings above. Whereas stories about Sanders were most trusted, those about Clinton were seen as most fair (0.67 average).

% \begin{figure}[H] 
% \centering 
%   \includegraphics[width=.49\textwidth]{fair_clinton}   
%   \includegraphics[width=.49\textwidth]{fair_sanders}   
%   \includegraphics[width=.49\textwidth]{fair_trump}   
%   \includegraphics[width=.49\textwidth]{fair_cruz}  
%   \caption{Fairness Ratings for Stories About Candidate
%     \label{fig:fair-candidate}}   
% \end{figure}

Women were more likely to see articles as fair on average (0.67) than men (0.51 mean).
Again, those in age group 30-49 were also most likely to find stories fair (average .70), this time followed by Millenials (0.51 average), then those 50-64 (0.48) and 64+ (0.20).

Democrats and Independents were far more likely to view stories as fair on average (0.68, 0.67) than Republicans (0.375), although all averages were between neutral and fair.

This finding about low perceptions of fairness in media is aligned with prior research findings. In ``The Liberal Media Myth Revisited,'' T.T. Lee hypothesized (and verified) that ``the more conservative (versus liberal) media consumers are, the more likely they are to perceive a media bias'' and similarly ``the more consumers lean toward the Republican (versus Democratic) party, the more likely they are to perceive a media bias'' \cite{lee2005liberal}.
% For distributions of trust by age group, see figure~\ref{fig:trust-by-age} on page \pageref{fig:trust-by-age}.

% On average, stories were more trusted by democrats (0.69 mean).

% % FIX THIS !!!!!!!!!!
% For distributions of trust by party affiliation, see figure ?? on page ??.
 

%% FUN SECTION: COMMENTS COLLECTED

