\chapter{Reading Level Effects}

\section{Motivation}

What was the purpose of the study?
What were the hypotheses? 

\section{Experimental Design}
\subsection{Quality Control}

CrowdFlower has a built-in ``Test Question'' feature that allows for the rejection of a annotator whose answers to specific questions do not lie within a threshold (default 70\%) of the ``correct'' answer or whose answers lay outside the standard variation compared to others.

However, since the questions we asked were by nature subjective and therefore outliers and disagreements in answers could imply signal rather than noise, we chose to monitor for quality using other metrics instead. CrowdFlower was not designed explicitly for survey-like tasks, and therefore there were no options for different screening methods or questions. Gold Questions on the platform are selected by the creator within the set of all questions being recorded.

Because of this, we monitored quality of results in two ways:

First, by setting a minimum of time of 180 seconds to complete the task of reading 5 stories for a task to be accepted,

And second, by selecting only Level 3 contributors on CrowdFlower as suggested on their website for handling survey-like tasks \cite{CrowdFlower-guide}.

Level 3 contributors are described as those who ``have completed over a hundred Test Questions across hundreds of different Job types, and have a near perfect overall Accuracy'' \cite{Crowdflower-levels}. This is the highest category of contributor.
 
Users were also only allowed to answer the set of questions once. 

Average response time was 07:31 minutes.
\$0.50 was given per survey, as suggested by MIT Committee on the Use of Humans as Experimental Subjects \cite{COUHES-turk}.
 

\subsection{Dataset}
\subsection{Survey} 
\section{Analysis}
\section{Conclusions}

\section{Limitations}

From our exploratory study on reading level effects, we were able to obtain a significant but weak effect between disclosing the source and the levels of trust marked by readers towards an article.

We also observed trends that suggested an interaction between disclosing the source and the reading level of a story.

However, the study faced several limitations: first, we did not obtain enough samples to show a statistically significant result for interactions between source and reading level.

Furthermore, multiple levels of independent variables (ie: 5 levels for input source) made modeling complex and the results less clear.

The dataset was also unbalanced and sparse (ie, because of large numbers of input variables we did not have complete representation for each category, such as high, low, and mid-reading level stories for every outlet and topic). We tried to control for those factors by randomization, however it made more difficult to analyze specific correlations between source and trust.

To further explore the interaction between disclosing the source and the reading level of the story, we set up another crowdsourcing experiment on CrowdFlower, this time targeting this specific interaction, to see if there is a significant effect between the two, detailed in the following chapter.